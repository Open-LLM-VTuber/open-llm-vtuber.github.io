"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1102],{3172:(e,l,n)=>{n.r(l),n.d(l,{assets:()=>d,contentTitle:()=>c,default:()=>p,frontMatter:()=>r,metadata:()=>a,toc:()=>u});const a=JSON.parse('{"id":"user-guide/backend/llm","title":"Language Models (LLM)","description":"This project supports multiple large language model backends and models.","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/user-guide/backend/llm.md","sourceDirName":"user-guide/backend","slug":"/user-guide/backend/llm","permalink":"/en/docs/user-guide/backend/llm","draft":false,"unlisted":false,"editUrl":"https://github.com/Open-LLM-VTuber/Open-LLM-VTuber-Docs/tree/main/docs/user-guide/backend/llm.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"docSidebar","previous":{"title":"Speech Recognition (ASR)","permalink":"/en/docs/user-guide/backend/asr"},"next":{"title":"Agent","permalink":"/en/docs/user-guide/backend/agent"}}');var i=n(4848),t=n(8453),o=n(5537),s=n(9329);const r={sidebar_position:4},c="Language Models (LLM)",d={},u=[{value:"How to configure and switch between different large language model backends",id:"how-to-configure-and-switch-between-different-large-language-model-backends",level:3},{value:"1. Configure large language model settings",id:"1-configure-large-language-model-settings",level:4},{value:"2. Switch to the corresponding large language model (LLM) in the settings of the respective agent",id:"2-switch-to-the-corresponding-large-language-model-llm-in-the-settings-of-the-respective-agent",level:4},{value:"Supported Large Language Model Backends",id:"supported-large-language-model-backends",level:2},{value:"OpenAI Compatible API (<code>openai_compatible_llm</code>)",id:"openai-compatible-api-openai_compatible_llm",level:3},{value:"Configuration Instructions",id:"configuration-instructions",level:4},{value:"Ollama (<code>ollama_llm</code>)",id:"ollama-ollama_llm",level:3},{value:"Ollama Installation Guide",id:"ollama-installation-guide",level:4},{value:"Modify Configuration File",id:"modify-configuration-file",level:4},{value:"OpenAI Official API (<code>openai_llm</code>)",id:"openai-official-api-openai_llm",level:3},{value:"Gemini API (<code>gemini_llm</code>)",id:"gemini-api-gemini_llm",level:3},{value:"Zhipu API (<code>zhipu_llm</code>)",id:"zhipu-api-zhipu_llm",level:3},{value:"DeepSeek API (<code>deepseek</code>)",id:"deepseek-api-deepseek",level:3},{value:"Mistral API (<code>mistral_llm</code>)",id:"mistral-api-mistral_llm",level:3},{value:"Groq API (<code>groq_llm</code>)",id:"groq-api-groq_llm",level:3},{value:"Claude (<code>claude_llm</code>)",id:"claude-claude_llm",level:3},{value:"LLama CPP (<code>llama_cpp_llm</code>)",id:"llama-cpp-llama_cpp_llm",level:3},{value:"Device Requirements",id:"device-requirements",level:4},{value:"Installation",id:"installation",level:3}];function h(e){const l={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(l.header,{children:(0,i.jsx)(l.h1,{id:"language-models-llm",children:"Language Models (LLM)"})}),"\n",(0,i.jsx)(l.p,{children:"This project supports multiple large language model backends and models."}),"\n",(0,i.jsxs)(l.admonition,{type:"note",children:[(0,i.jsxs)(l.p,{children:["Almost all large language model (LLM) APIs and inference engines support the OpenAI format. So, if you find that an LLM API you want to use isn't explicitly supported in our project, you can usually just fill in the relevant information (base URL, API key, model name) into the ",(0,i.jsx)(l.code,{children:"openai_compatible_llm"})," section, and it should work right away."]}),(0,i.jsxs)(l.p,{children:["In fact, apart from llama.cpp and Claude, all other LLM APIs or backends supported by this project are essentially just variations of ",(0,i.jsx)(l.code,{children:"openai_compatible_llm"})," (we added some model loading logic for Ollama), using the exact same code. The only difference is that we've pre-filled the base URL and some other settings for you."]})]}),"\n",(0,i.jsx)(l.h3,{id:"how-to-configure-and-switch-between-different-large-language-model-backends",children:"How to configure and switch between different large language model backends"}),"\n",(0,i.jsxs)(l.blockquote,{children:["\n",(0,i.jsxs)(l.p,{children:["The project's default agent is ",(0,i.jsx)(l.code,{children:"basic_memory_agent"}),", so to switch the language model for the default agent, make selections under the ",(0,i.jsx)(l.code,{children:"llm_provider"})," option of ",(0,i.jsx)(l.code,{children:"basic_memory_agent"}),"."]}),"\n"]}),"\n",(0,i.jsx)(l.h4,{id:"1-configure-large-language-model-settings",children:"1. Configure large language model settings"}),"\n",(0,i.jsxs)(l.p,{children:["Refer to the ",(0,i.jsx)(l.a,{href:"#supported-large-language-model-backends",children:"Supported Large Language Model Backends"})," section below to configure the corresponding large language model backend."]}),"\n",(0,i.jsxs)(l.p,{children:["For example, if you want to use Ollama, please follow the guide in the ",(0,i.jsx)(l.a,{href:"#ollama-ollama_llm",children:"Ollama"})," section to install and configure Ollama-related settings."]}),"\n",(0,i.jsxs)(l.p,{children:["Under ",(0,i.jsx)(l.code,{children:"agent_config"})," in ",(0,i.jsx)(l.code,{children:"llm_config"}),", you can configure the connection settings for the backend and various LLMs."]}),"\n",(0,i.jsx)(l.h4,{id:"2-switch-to-the-corresponding-large-language-model-llm-in-the-settings-of-the-respective-agent",children:"2. Switch to the corresponding large language model (LLM) in the settings of the respective agent"}),"\n",(0,i.jsxs)(l.blockquote,{children:["\n",(0,i.jsx)(l.p,{children:"Some agents may not support custom LLMs"}),"\n"]}),"\n",(0,i.jsxs)(l.p,{children:["Go to the ",(0,i.jsx)(l.code,{children:"basic_memory_agent"})," settings:"]}),"\n",(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-yaml",children:'basic_memory_agent:\n    # "openai_compatible_llm", "llama_cpp_llm", "claude_llm", "ollama_llm"\n    # "openai_llm", "gemini_llm", "zhipu_llm", "deepseek_llm", "groq_llm"\n    # "mistral_llm"\n    llm_provider: "openai_compatible_llm" # LLM solution to use\n    faster_first_response: True\n'})}),"\n",(0,i.jsxs)(l.p,{children:["Replace ",(0,i.jsx)(l.code,{children:"basic_memory_agent"})," with the large language model (LLM) you want to use."]}),"\n",(0,i.jsxs)(l.p,{children:["Note that ",(0,i.jsx)(l.code,{children:"llm_provider"})," can only be filled with large language model backends that exist under ",(0,i.jsx)(l.code,{children:"llm_configs"}),", such as ",(0,i.jsx)(l.code,{children:"openai_compatible_llm"}),", ",(0,i.jsx)(l.code,{children:"claude_llm"}),", etc."]}),"\n",(0,i.jsx)(l.h2,{id:"supported-large-language-model-backends",children:"Supported Large Language Model Backends"}),"\n",(0,i.jsxs)(l.h3,{id:"openai-compatible-api-openai_compatible_llm",children:["OpenAI Compatible API (",(0,i.jsx)(l.code,{children:"openai_compatible_llm"}),")"]}),"\n",(0,i.jsx)(l.p,{children:"Compatible with all API endpoints that support the OpenAI Chat Completion format. This includes LM Studio, vLLM, and most inference tools and API providers."}),"\n",(0,i.jsxs)(l.p,{children:["The later official OpenAI API, Gemini, Zhipu, DeepSeek, Mistral, and Groq are all wrappers of ",(0,i.jsx)(l.code,{children:"openai_compatible_llm"})," (Ollama is also a wrapper, but with added special memory management mechanisms). I've just pre-filled the correct ",(0,i.jsx)(l.code,{children:"base_url"})," and related configurations for you."]}),"\n",(0,i.jsx)(l.h4,{id:"configuration-instructions",children:"Configuration Instructions"}),"\n",(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-yaml",children:'# OpenAI compatible inference backend\nopenai_compatible_llm:\n    base_url: "http://localhost:11434/v1" # Base URL\n    llm_api_key: "somethingelse" # API key\n    organization_id: "org_eternity" # Organization ID\n    project_id: "project_glass" # Project ID\n    model: "qwen2.5:latest" # Model to use\n    temperature: 1.0 # Temperature, between 0 and 2\n'})}),"\n",(0,i.jsxs)(l.h3,{id:"ollama-ollama_llm",children:["Ollama (",(0,i.jsx)(l.code,{children:"ollama_llm"}),")"]}),"\n",(0,i.jsx)(l.p,{children:"Ollama is a popular LLM inference tool that allows easy downloading and running of large language models."}),"\n",(0,i.jsx)(l.h4,{id:"ollama-installation-guide",children:"Ollama Installation Guide"}),"\n",(0,i.jsxs)(l.ol,{children:["\n",(0,i.jsxs)(l.li,{children:["Download and install from the ",(0,i.jsx)(l.a,{href:"https://ollama.com/",children:"Ollama official website"})]}),"\n",(0,i.jsx)(l.li,{children:"Verify installation:"}),"\n"]}),"\n",(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-bash",children:"ollama --version\n"})}),"\n",(0,i.jsxs)(l.ol,{start:"3",children:["\n",(0,i.jsxs)(l.li,{children:["Download and run the model (using ",(0,i.jsx)(l.code,{children:"qwen2.5:latest"})," as an example):"]}),"\n"]}),"\n",(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-bash",children:"ollama run qwen2.5:latest\n# After successful execution, you can directly chat with qwen2.5:latest\n# You can exit the chat interface (Ctrl/Command + D), but don't close the command line\n"})}),"\n",(0,i.jsxs)(l.ol,{start:"4",children:["\n",(0,i.jsx)(l.li,{children:"View installed models:"}),"\n"]}),"\n",(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-bash",children:"ollama list\n# NAME                ID              SIZE      MODIFIED\n# qwen2.5:latest      845dbda0ea48    4.7 GB    2 minutes ago\n"})}),"\n",(0,i.jsx)(l.admonition,{type:"tip",children:(0,i.jsxs)(l.p,{children:["When looking for model names, please use the ",(0,i.jsx)(l.code,{children:"ollama list"})," command to check the models downloaded in ollama, and directly copy and paste the model name under the ",(0,i.jsx)(l.code,{children:"model"})," option to avoid issues such as misspelling the model name, full-width colons, spaces, etc."]})}),"\n",(0,i.jsx)(l.admonition,{type:"caution",children:(0,i.jsx)(l.p,{children:"When selecting a model, please consider your VRAM capacity and GPU computing power. If the model file size is larger than the VRAM capacity, the model will be forced to use CPU computation, which is extremely slow. Additionally, the smaller the model parameter count, the lower the conversation latency. If you want to reduce conversation latency, please choose a model with fewer parameters."})}),"\n",(0,i.jsx)(l.h4,{id:"modify-configuration-file",children:"Modify Configuration File"}),"\n",(0,i.jsxs)(l.p,{children:["Edit ",(0,i.jsx)(l.code,{children:"conf.yaml"}),":"]}),"\n",(0,i.jsxs)(l.ol,{children:["\n",(0,i.jsxs)(l.li,{children:["Set ",(0,i.jsx)(l.code,{children:"llm_provider"})," under ",(0,i.jsx)(l.code,{children:"basic_memory_agent"})," to ",(0,i.jsx)(l.code,{children:"ollama_llm"})]}),"\n",(0,i.jsxs)(l.li,{children:["Adjust the settings under ",(0,i.jsx)(l.code,{children:"ollama_llm"})," in the ",(0,i.jsx)(l.code,{children:"llm_configs"})," option:","\n",(0,i.jsxs)(l.ul,{children:["\n",(0,i.jsxs)(l.li,{children:["Keep ",(0,i.jsx)(l.code,{children:"base_url"})," default for local running, no need to modify."]}),"\n",(0,i.jsxs)(l.li,{children:["Set ",(0,i.jsx)(l.code,{children:"model"})," to the model you're using, such as ",(0,i.jsx)(l.code,{children:"qwen2.5:latest"})," used in this guide."]}),"\n"]}),"\n",(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-yaml",children:"ollama_llm:\n  base_url: http://localhost:11434  # Keep default for local running\n  model: qwen2.5:latest            # Model name obtained from ollama list\n  temperature: 0.7                 # Controls answer randomness, higher is more random (0~1)\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(l.h3,{id:"openai-official-api-openai_llm",children:["OpenAI Official API (",(0,i.jsx)(l.code,{children:"openai_llm"}),")"]}),"\n",(0,i.jsx)(l.p,{children:"First, obtain an API key from the OpenAI official website."}),"\n",(0,i.jsx)(l.p,{children:"Then adjust the settings here:"}),"\n",(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-yaml",children:'openai_llm:\n    llm_api_key: "Your Open AI API key" # OpenAI API key\n    model: "gpt-4o" # Model to use\n    temperature: 1.0 # Temperature, between 0 and 2\n'})}),"\n",(0,i.jsxs)(l.h3,{id:"gemini-api-gemini_llm",children:["Gemini API (",(0,i.jsx)(l.code,{children:"gemini_llm"}),")"]}),"\n",(0,i.jsxs)(l.p,{children:["Go to ",(0,i.jsx)(l.a,{href:"https://aistudio.google.com/",children:"Google AI Studio"})," to generate an API key."]}),"\n",(0,i.jsx)(l.p,{children:"Then adjust the settings here:"}),"\n",(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-yaml",children:'gemini_llm:\n    llm_api_key: "Your Gemini API Key" # Gemini API key\n    model: "gemini-2.0-flash-exp" # Model to use\n    temperature: 1.0 # Temperature, between 0 and 2\n'})}),"\n",(0,i.jsxs)(l.h3,{id:"zhipu-api-zhipu_llm",children:["Zhipu API (",(0,i.jsx)(l.code,{children:"zhipu_llm"}),")"]}),"\n",(0,i.jsxs)(l.p,{children:["Go to ",(0,i.jsx)(l.a,{href:"https://bigmodel.cn/",children:"Zhipu"})," to obtain an API key."]}),"\n",(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-yaml",children:'zhipu_llm:\n    llm_api_key: "Your ZhiPu AI API key" # Zhipu AI API key\n    model: "glm-4-flash" # Model to use\n    temperature: 1.0 # Temperature, between 0 and 2\n'})}),"\n",(0,i.jsxs)(l.h3,{id:"deepseek-api-deepseek",children:["DeepSeek API (",(0,i.jsx)(l.code,{children:"deepseek"}),")"]}),"\n",(0,i.jsxs)(l.p,{children:["Go to ",(0,i.jsx)(l.a,{href:"#deepseek-api-deepseek",children:"DeepSeek"})," to obtain an API key."]}),"\n",(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-yaml",children:'zhipu_llm:\n    llm_api_key: "Your ZhiPu AI API key" # Zhipu AI API key\n    model: "glm-4-flash" # Model to use\n    temperature: 1.0 # Temperature, between 0 and 2\n'})}),"\n",(0,i.jsxs)(l.h3,{id:"mistral-api-mistral_llm",children:["Mistral API (",(0,i.jsx)(l.code,{children:"mistral_llm"}),")"]}),"\n",(0,i.jsxs)(l.p,{children:["Go to the ",(0,i.jsx)(l.a,{href:"https://example.com",children:"Mistral official website"})," to obtain an API key."]}),"\n",(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-yaml",children:'mistral_llm:\n    llm_api_key: "Your Mistral API key" # Mistral API key\n    model: "pixtral-large-latest" # Model to use\n    temperature: 1.0 # Temperature, between 0 and 2\n'})}),"\n",(0,i.jsxs)(l.h3,{id:"groq-api-groq_llm",children:["Groq API (",(0,i.jsx)(l.code,{children:"groq_llm"}),")"]}),"\n",(0,i.jsxs)(l.p,{children:["Go to the ",(0,i.jsx)(l.a,{href:"https://console.groq.com/keys",children:"Groq official website"})," to obtain an API key."]}),"\n",(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-yaml",children:'groq_llm:\n    llm_api_key: "your groq API key" # Groq API key\n    model: "llama-3.3-70b-versatile" # Model to use\n    temperature: 1.0 # Temperature, between 0 and 2\n'})}),"\n",(0,i.jsxs)(l.h3,{id:"claude-claude_llm",children:["Claude (",(0,i.jsx)(l.code,{children:"claude_llm"}),")"]}),"\n",(0,i.jsxs)(l.p,{children:["In ",(0,i.jsx)(l.a,{href:"https://github.com/t41372/Open-LLM-VTuber/pull/35",children:"https://github.com/t41372/Open-LLM-VTuber/pull/35"}),", version ",(0,i.jsx)(l.code,{children:"v0.3.1"})," added support for Claude."]}),"\n",(0,i.jsxs)(l.p,{children:["Change ",(0,i.jsx)(l.code,{children:"LLM_PROVIDER"})," to ",(0,i.jsx)(l.code,{children:"claude"})," and complete the settings under ",(0,i.jsx)(l.code,{children:"claude"}),"."]}),"\n",(0,i.jsxs)(l.h3,{id:"llama-cpp-llama_cpp_llm",children:["LLama CPP (",(0,i.jsx)(l.code,{children:"llama_cpp_llm"}),")"]}),"\n",(0,i.jsxs)(l.p,{children:["llama cpp provides a way to run LLM (gguf files) directly ",(0,i.jsx)(l.strong,{children:"within this project"})," without any external tools (such as Ollama) and without starting any additional servers. You only need a ",(0,i.jsx)(l.code,{children:".gguf"})," model file."]}),"\n",(0,i.jsx)(l.h4,{id:"device-requirements",children:"Device Requirements"}),"\n",(0,i.jsxs)(l.p,{children:["According to the ",(0,i.jsx)(l.a,{href:"https://github.com/abetlen/llama-cpp-python",children:"project repository"})]}),"\n",(0,i.jsx)(l.p,{children:"Requirements:"}),"\n",(0,i.jsxs)(l.ul,{children:["\n",(0,i.jsx)(l.li,{children:"Python 3.8+"}),"\n",(0,i.jsxs)(l.li,{children:["C compiler","\n",(0,i.jsxs)(l.ul,{children:["\n",(0,i.jsx)(l.li,{children:"Linux: gcc or clang"}),"\n",(0,i.jsx)(l.li,{children:"Windows: Visual Studio or MinGW"}),"\n",(0,i.jsx)(l.li,{children:"MacOS: Xcode"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(l.p,{children:["During installation, ",(0,i.jsx)(l.code,{children:"llama.cpp"})," will be built from source and installed along with this Python package."]}),"\n",(0,i.jsxs)(l.p,{children:["If it fails later, please add ",(0,i.jsx)(l.code,{children:"--verbose"})," to the ",(0,i.jsx)(l.code,{children:"pip install"})," command to view the full cmake build log."]}),"\n",(0,i.jsx)(l.h3,{id:"installation",children:"Installation"}),"\n",(0,i.jsx)(l.p,{children:"Please run the command in the project directory according to your device."}),"\n",(0,i.jsxs)(o.A,{children:[(0,i.jsx)(s.A,{value:"nvidia",label:"Nvidia GPU",children:(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-bash",children:'CMAKE_ARGS="-DGGML_CUDA=on" uv pip install llama-cpp-python\n'})})}),(0,i.jsx)(s.A,{value:"apple",label:"Apple Silicon Mac",children:(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-bash",children:'CMAKE_ARGS="-DGGML_METAL=on" uv pip install llama-cpp-python\n'})})}),(0,i.jsx)(s.A,{value:"amd",label:"AMD GPU (ROCm)",children:(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-bash",children:'CMAKE_ARGS="-DGGML_HIPBLAS=on" uv pip install llama-cpp-python\n'})})}),(0,i.jsx)(s.A,{value:"cpu",label:"CPU (OpenBlas)",children:(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{className:"language-bash",children:'CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" uv pip install llama-cpp-python\n'})})})]}),"\n",(0,i.jsxs)(l.p,{children:["If you can't find your device above, you can look for the ",(0,i.jsx)(l.code,{children:"pip install llama-cpp-python"})," command suitable for your platform ",(0,i.jsx)(l.a,{href:"https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#supported-backends",children:"here"}),"."]}),"\n",(0,i.jsx)(l.admonition,{type:"warning",children:(0,i.jsxs)(l.p,{children:["All ",(0,i.jsx)(l.code,{children:"pip"})," commands should be changed to ",(0,i.jsx)(l.code,{children:"uv pip"}),", so that they will be installed in the project's virtual environment. For example, if the project page says ",(0,i.jsx)(l.code,{children:"pip install llama-cpp-python"}),", you should change it to ",(0,i.jsx)(l.code,{children:"uv pip install llama-cpp-python"})]})}),"\n",(0,i.jsxs)(l.p,{children:["If you encounter problems at this step, you can check out the ",(0,i.jsx)(l.a,{href:"https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#windows-notes",children:"Windows Note"})," and ",(0,i.jsx)(l.a,{href:"https://github.com/abetlen/llama-cpp-python?tab=readme-ov-file#macos-notes",children:"macOS Note"})]}),"\n",(0,i.jsx)(l.pre,{children:(0,i.jsx)(l.code,{children:"\n\n"})})]})}function p(e={}){const{wrapper:l}={...(0,t.R)(),...e.components};return l?(0,i.jsx)(l,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},9329:(e,l,n)=>{n.d(l,{A:()=>o});n(6540);var a=n(4164);const i={tabItem:"tabItem_Ymn6"};var t=n(4848);function o(e){let{children:l,hidden:n,className:o}=e;return(0,t.jsx)("div",{role:"tabpanel",className:(0,a.A)(i.tabItem,o),hidden:n,children:l})}},5537:(e,l,n)=>{n.d(l,{A:()=>v});var a=n(6540),i=n(4164),t=n(5627),o=n(6347),s=n(372),r=n(604),c=n(1861),d=n(8749);function u(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:l}=e;return!!l&&"object"==typeof l&&"value"in l}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function h(e){const{values:l,children:n}=e;return(0,a.useMemo)((()=>{const e=l??function(e){return u(e).map((e=>{let{props:{value:l,label:n,attributes:a,default:i}}=e;return{value:l,label:n,attributes:a,default:i}}))}(n);return function(e){const l=(0,c.XI)(e,((e,l)=>e.value===l.value));if(l.length>0)throw new Error(`Docusaurus error: Duplicate values "${l.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[l,n])}function p(e){let{value:l,tabValues:n}=e;return n.some((e=>e.value===l))}function m(e){let{queryString:l=!1,groupId:n}=e;const i=(0,o.W6)(),t=function(e){let{queryString:l=!1,groupId:n}=e;if("string"==typeof l)return l;if(!1===l)return null;if(!0===l&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:l,groupId:n});return[(0,r.aZ)(t),(0,a.useCallback)((e=>{if(!t)return;const l=new URLSearchParams(i.location.search);l.set(t,e),i.replace({...i.location,search:l.toString()})}),[t,i])]}function g(e){const{defaultValue:l,queryString:n=!1,groupId:i}=e,t=h(e),[o,r]=(0,a.useState)((()=>function(e){let{defaultValue:l,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(l){if(!p({value:l,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${l}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return l}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:l,tabValues:t}))),[c,u]=m({queryString:n,groupId:i}),[g,x]=function(e){let{groupId:l}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(l),[i,t]=(0,d.Dv)(n);return[i,(0,a.useCallback)((e=>{n&&t.set(e)}),[n,t])]}({groupId:i}),f=(()=>{const e=c??g;return p({value:e,tabValues:t})?e:null})();(0,s.A)((()=>{f&&r(f)}),[f]);return{selectedValue:o,selectValue:(0,a.useCallback)((e=>{if(!p({value:e,tabValues:t}))throw new Error(`Can't select invalid tab value=${e}`);r(e),u(e),x(e)}),[u,x,t]),tabValues:t}}var x=n(9136);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var j=n(4848);function b(e){let{className:l,block:n,selectedValue:a,selectValue:o,tabValues:s}=e;const r=[],{blockElementScrollPositionUntilNextRender:c}=(0,t.a_)(),d=e=>{const l=e.currentTarget,n=r.indexOf(l),i=s[n].value;i!==a&&(c(l),o(i))},u=e=>{let l=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const n=r.indexOf(e.currentTarget)+1;l=r[n]??r[0];break}case"ArrowLeft":{const n=r.indexOf(e.currentTarget)-1;l=r[n]??r[r.length-1];break}}l?.focus()};return(0,j.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":n},l),children:s.map((e=>{let{value:l,label:n,attributes:t}=e;return(0,j.jsx)("li",{role:"tab",tabIndex:a===l?0:-1,"aria-selected":a===l,ref:e=>{r.push(e)},onKeyDown:u,onClick:d,...t,className:(0,i.A)("tabs__item",f.tabItem,t?.className,{"tabs__item--active":a===l}),children:n??l},l)}))})}function _(e){let{lazy:l,children:n,selectedValue:t}=e;const o=(Array.isArray(n)?n:[n]).filter(Boolean);if(l){const e=o.find((e=>e.props.value===t));return e?(0,a.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,j.jsx)("div",{className:"margin-top--md",children:o.map(((e,l)=>(0,a.cloneElement)(e,{key:l,hidden:e.props.value!==t})))})}function y(e){const l=g(e);return(0,j.jsxs)("div",{className:(0,i.A)("tabs-container",f.tabList),children:[(0,j.jsx)(b,{...l,...e}),(0,j.jsx)(_,{...l,...e})]})}function v(e){const l=(0,x.A)();return(0,j.jsx)(y,{...e,children:u(e.children)},String(l))}},8453:(e,l,n)=>{n.d(l,{R:()=>o,x:()=>s});var a=n(6540);const i={},t=a.createContext(i);function o(e){const l=a.useContext(t);return a.useMemo((function(){return"function"==typeof e?e(l):{...l,...e}}),[l,e])}function s(e){let l;return l=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(t.Provider,{value:l},e.children)}}}]);