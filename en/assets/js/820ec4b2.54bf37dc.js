"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9421],{1082:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>h});const s=JSON.parse('{"id":"user-guide/backend/asr","title":"Speech Recognition (ASR)","description":"Speech Recognition (ASR, Automatic Speech Recognition) converts user speech to text. This project supports multiple speech recognition model implementations.","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/user-guide/backend/asr.md","sourceDirName":"user-guide/backend","slug":"/user-guide/backend/asr","permalink":"/en/docs/user-guide/backend/asr","draft":false,"unlisted":false,"editUrl":"https://github.com/Open-LLM-VTuber/Open-LLM-VTuber-Docs/tree/main/docs/user-guide/backend/asr.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docSidebar","previous":{"title":"Configuration file","permalink":"/en/docs/user-guide/backend/config"},"next":{"title":"Language Models (LLM)","permalink":"/en/docs/user-guide/backend/llm"}}');var i=r(4848),o=r(8453),l=r(5537),t=r(9329);const a={sidebar_position:3},c="Speech Recognition (ASR)",d={},h=[{value:"<code>sherpa_onnx_asr</code> (Local &amp; Project Default)",id:"sherpa_onnx_asr-local--project-default",level:2},{value:"Recommended Users",id:"recommended-users",level:3},{value:"CUDA Inference",id:"cuda-inference",level:3},{value:"Using Other sherpa-onnx Models",id:"using-other-sherpa-onnx-models",level:3},{value:"<code>fun_asr</code> (Local)",id:"fun_asr-local",level:2},{value:"Recommended Users",id:"recommended-users-1",level:3},{value:"Installation",id:"installation",level:3},{value:"<code>faster_whisper</code> (Local)",id:"faster_whisper-local",level:2},{value:"Recommended Users",id:"recommended-users-2",level:3},{value:"Installation and Configuration",id:"installation-and-configuration",level:3},{value:"Model Selection (model_path)",id:"model-selection-model_path",level:3},{value:"<code>whisper_cpp</code> (Local)",id:"whisper_cpp-local",level:2},{value:"Recommended Users",id:"recommended-users-3",level:3},{value:"Installation",id:"installation-1",level:3},{value:"CoreML Configuration",id:"coreml-configuration",level:3},{value:"<code>whisper</code> (Local)",id:"whisper-local",level:2},{value:"Recommended Users",id:"recommended-users-4",level:3},{value:"<code>groq_whisper_asr</code> (Online, requires API key, but easy to register with generous free quota)",id:"groq_whisper_asr-online-requires-api-key-but-easy-to-register-with-generous-free-quota",level:2},{value:"Recommended Users",id:"recommended-users-5",level:3},{value:"<code>azure_asr</code> (Online, requires API key)",id:"azure_asr-online-requires-api-key",level:2},{value:"Recommended Users",id:"recommended-users-6",level:3}];function u(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"speech-recognition-asr",children:"Speech Recognition (ASR)"})}),"\n",(0,i.jsx)(n.p,{children:"Speech Recognition (ASR, Automatic Speech Recognition) converts user speech to text. This project supports multiple speech recognition model implementations."}),"\n",(0,i.jsxs)(n.p,{children:["ASR-related configuration items are under ",(0,i.jsx)(n.code,{children:"asr_config"})," in ",(0,i.jsx)(n.code,{children:"conf.yaml"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"Here are the speech recognition options you can choose from:"}),"\n",(0,i.jsxs)(n.h2,{id:"sherpa_onnx_asr-local--project-default",children:[(0,i.jsx)(n.code,{children:"sherpa_onnx_asr"})," (Local & Project Default)"]}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["(Added in ",(0,i.jsx)(n.code,{children:"v0.5.0-alpha.1"})," PR: ",(0,i.jsx)(n.a,{href:"https://github.com/t41372/Open-LLM-VTuber/pull/50",children:"Add sherpa-onnx support #50"}),")"]})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://github.com/k2-fsa/sherpa-onnx",children:"sherpa-onnx"})," is a feature-rich inference tool that can run various speech recognition (ASR) models."]}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["Starting from version ",(0,i.jsx)(n.code,{children:"v1.0.0"}),", this project uses ",(0,i.jsx)(n.code,{children:"sherpa-onnx"})," to run the ",(0,i.jsx)(n.code,{children:"SenseVoiceSmall"})," (int8 quantized) model as the default speech recognition solution. This is an out-of-the-box configuration - you don't need any additional setup. The system will automatically download and extract model files to the project's ",(0,i.jsx)(n.code,{children:"models"})," directory on first run."]})}),"\n",(0,i.jsx)(n.h3,{id:"recommended-users",children:"Recommended Users"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"All users (hence it's the default)"}),"\n",(0,i.jsx)(n.li,{children:"Especially Mac users (due to limited options)"}),"\n",(0,i.jsx)(n.li,{children:"Non-NVIDIA GPU users"}),"\n",(0,i.jsx)(n.li,{children:"Chinese users"}),"\n",(0,i.jsx)(n.li,{children:"Fast CPU inference"}),"\n",(0,i.jsx)(n.li,{children:"Configuration difficulty: No configuration needed as it's the project default"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The SenseVoiceSmall model may have average English performance."}),"\n",(0,i.jsx)(n.h3,{id:"cuda-inference",children:"CUDA Inference"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"sherpa-onnx"})," supports both CPU and CUDA inference. While the default ",(0,i.jsx)(n.code,{children:"SenseVoiceSmall"})," model performs well on CPU, if you have an NVIDIA GPU, you can enable CUDA inference for better performance by following these steps:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"First uninstall the CPU version dependencies:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sh",children:"uv remove sherpa-onnx onnxruntime\n# Avoid introducing onnxruntime through dependencies\nuv remove faster-whisper\n"})}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"Note that sherpa-onnx is installed via pre-built wheels in the example, which means you need to install"}),"\n",(0,i.jsxs)(n.p,{children:["CUDA Toolkit 11.x + CUDNN 8.x for CUDA 11.x (and add ",(0,i.jsx)(n.code,{children:"%SystemDrive%\\Program Files\\NVIDIA\\CUDNN\\v8.x\\bin"})," to your ",(0,i.jsx)(n.code,{children:"PATH"}),")"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["Where x is your cudnn minor version number, e.g., for version ",(0,i.jsx)(n.code,{children:"v8.9.7"}),", write ",(0,i.jsx)(n.code,{children:"v8.9"})," here."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"to link to the correct CUDA environment."}),"\n",(0,i.jsxs)(n.p,{children:["If you don't want to use the NVIDIA official installer/manually set PATH, consider using ",(0,i.jsx)(n.a,{href:"https://pixi.sh/",children:(0,i.jsx)(n.code,{children:"pixi"})})," to manage a local conda environment.\nThis approach doesn't require you to install dependencies via uv."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-nushell",children:"pixi remove --pypi onnxruntime sherpa-onnx\npixi add --pypi onnxruntime-gpu==1.17.1 pip\npixi run python -m pip install sherpa-onnx==1.10.39+cuda -f https://k2-fsa.github.io/sherpa/onnx/cuda.html\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsxs)(n.li,{children:["Install CUDA version of ",(0,i.jsx)(n.code,{children:"sherpa-onnx"})," and ",(0,i.jsx)(n.code,{children:"onnxruntime-gpu"})," dependencies:"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sh",children:"# sherpa-onnx provided pre-built wheels are compatible with onnxruntime-gpu==1.17.1\nuv add onnxruntime-gpu==1.17.1 sherpa-onnx==1.10.39+cuda -f https://k2-fsa.github.io/sherpa/onnx/cuda.html \n"})}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsxs)(n.li,{children:["Modify configuration file:\nIn ",(0,i.jsx)(n.code,{children:"conf.yaml"}),", find the ",(0,i.jsx)(n.code,{children:"sherpa_onnx_asr"})," section and set ",(0,i.jsx)(n.code,{children:"provider"})," to ",(0,i.jsx)(n.code,{children:"cuda"})]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"using-other-sherpa-onnx-models",children:"Using Other sherpa-onnx Models"}),"\n",(0,i.jsx)(n.p,{children:"If you want to try other speech recognition models:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Download the required model from ",(0,i.jsx)(n.a,{href:"https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models",children:"sherpa-onnx ASR models"})]}),"\n",(0,i.jsxs)(n.li,{children:["Place the model files in the project's ",(0,i.jsx)(n.code,{children:"models"})," directory"]}),"\n",(0,i.jsxs)(n.li,{children:["Modify the relevant configuration of ",(0,i.jsx)(n.code,{children:"sherpa_onnx_asr"})," according to the instructions in ",(0,i.jsx)(n.code,{children:"conf.yaml"})]}),"\n"]}),"\n",(0,i.jsxs)(n.h2,{id:"fun_asr-local",children:[(0,i.jsx)(n.code,{children:"fun_asr"})," (Local)"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://github.com/modelscope/FunASR?tab=readme-ov-file",children:"FunASR"})," is a fundamental end-to-end speech recognition toolkit from ModelScope that supports various ASR models. Among them, Alibaba's ",(0,i.jsx)(n.a,{href:"https://github.com/FunAudioLLM/SenseVoice",children:"FunAudioLLM"})," SenseVoiceSmall model performs well in both performance and speed."]}),"\n",(0,i.jsxs)(n.admonition,{type:"tip",children:[(0,i.jsxs)(n.p,{children:["Although FunASR can run the SenseVoiceSmall model, we recommend using the project's default ",(0,i.jsx)(n.code,{children:"sherpa_onnx_asr"}),". The FunASR project has some stability issues and may encounter exceptions on certain devices."]}),(0,i.jsx)(n.p,{children:"However, FunASR utilizes GPU better, so it might be faster for NVIDIA GPU users."})]}),"\n",(0,i.jsx)(n.h3,{id:"recommended-users-1",children:"Recommended Users"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Users with NVIDIA GPUs who want to utilize GPU inference for the SenseVoiceSmall model"}),"\n",(0,i.jsx)(n.li,{children:"Chinese users"}),"\n",(0,i.jsx)(n.li,{children:"Fast CPU inference"}),"\n",(0,i.jsx)(n.li,{children:"Configuration difficulty: Simple"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"SenseVoiceSmall may have average English performance."}),"\n",(0,i.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,i.jsx)(n.p,{children:"In the project directory, run:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sh",children:"uv add funasr modelscope huggingface_hub onnxconverter_common torch torchaudio onnx\n"})}),"\n",(0,i.jsxs)(n.admonition,{title:"Dependency Issue Solutions",type:"info",children:[(0,i.jsx)(n.p,{children:"If you encounter the following dependency issues:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sh",children:"help: `llvmlite` (v0.36.0) was included because `open-llm-vtuber` (v1.0.0a1) depends on `funasr` (v1.2.2) which depends on `umap-learn` (v0.5.7)\n      which depends on `pynndescent` (v0.5.13) which depends on `llvmlite`\n"})}),(0,i.jsx)(n.p,{children:"You can try using the following command instead:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sh",children:"uv pip install funasr modelscope huggingface_hub torch torchaudio onnx onnxconverter_common\n"})})]}),"\n",(0,i.jsxs)(n.admonition,{type:"warning",children:[(0,i.jsx)(n.p,{children:"Even if model files are already local, an internet connection is still required at startup."}),(0,i.jsxs)(n.p,{children:["Solution: Directly specify the local path of the model in the configuration, so no internet connection is needed during runtime. However, you need to download the model files in advance. See ",(0,i.jsx)(n.a,{href:"https://github.com/modelscope/FunASR/issues/1897",children:"FunASR Issue #1897"})," for details."]})]}),"\n",(0,i.jsxs)(n.h2,{id:"faster_whisper-local",children:[(0,i.jsx)(n.code,{children:"faster_whisper"})," (Local)"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/SYSTRAN/faster-whisper",children:"Official Repository"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This is an optimized Whisper inference engine that can run original Whisper and distilled Whisper models. It provides faster inference speed compared to the original Whisper but cannot automatically detect language."}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["Faster Whisper ",(0,i.jsx)(n.a,{href:"https://github.com/SYSTRAN/faster-whisper/issues/911",children:"does not support Mac GPU inference"})," and can only run on CPU with average performance. It's recommended for use on devices equipped with NVIDIA GPUs for optimal performance."]})}),"\n",(0,i.jsx)(n.h3,{id:"recommended-users-2",children:"Recommended Users"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Users with NVIDIA GPUs who want to utilize GPU inference for Whisper models"}),"\n",(0,i.jsx)(n.li,{children:"Non-Chinese users. Whisper series models have good multilingual support"}),"\n",(0,i.jsx)(n.li,{children:"CPU inference is relatively slow"}),"\n",(0,i.jsx)(n.li,{children:"Configuration difficulty: Simple"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"installation-and-configuration",children:"Installation and Configuration"}),"\n",(0,i.jsxs)(n.p,{children:["If you want to use GPU acceleration (NVIDIA GPU users only), you need to install the following NVIDIA dependency libraries. For detailed installation steps, please refer to ",(0,i.jsx)(n.a,{href:"/docs/quick-start.md",children:"Quick Start"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/cublas",children:"cuBLAS for CUDA 12"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/cudnn",children:"cuDNN 8 for CUDA 12"})}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["If you don't care much about running speed or have a powerful CPU, you can also set the ",(0,i.jsx)(n.code,{children:"device"})," parameter of ",(0,i.jsx)(n.code,{children:"faster-whisper"})," to ",(0,i.jsx)(n.code,{children:"cpu"})," in the ",(0,i.jsx)(n.code,{children:"conf.yaml"})," configuration file. This avoids the hassle of installing NVIDIA dependency libraries."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# Faster Whisper Configuration\nfaster_whisper:\n  model_path: 'large-v3-turbo' # Model path, model name, or HF hub model id\n  download_root: 'models/whisper' # Model download root directory\n  language: 'zh' # Language, en, zh or others. Leave empty for auto-detection\n  device: 'auto' # Device, cpu, cuda or auto. faster-whisper doesn't support mps\n  compute_type: 'int8'\n"})}),"\n",(0,i.jsx)(n.h3,{id:"model-selection-model_path",children:"Model Selection (model_path)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"model_path"})," can be filled with model name, local path of the model (if you downloaded it in advance), or model id on HuggingFace (must be a model already converted to CTranslate2 format)."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Available model names:"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"tiny"}),", ",(0,i.jsx)(n.code,{children:"tiny.en"}),", ",(0,i.jsx)(n.code,{children:"base"}),", ",(0,i.jsx)(n.code,{children:"base.en"}),", ",(0,i.jsx)(n.code,{children:"small"}),", ",(0,i.jsx)(n.code,{children:"small.en"}),", ",(0,i.jsx)(n.code,{children:"distil-small.en"}),", ",(0,i.jsx)(n.code,{children:"medium"}),", ",(0,i.jsx)(n.code,{children:"medium.en"}),", ",(0,i.jsx)(n.code,{children:"distil-medium.en"}),", ",(0,i.jsx)(n.code,{children:"large-v1"}),", ",(0,i.jsx)(n.code,{children:"large-v2"}),", ",(0,i.jsx)(n.code,{children:"large-v3"}),", ",(0,i.jsx)(n.code,{children:"large"}),", ",(0,i.jsx)(n.code,{children:"distil-large-v2"}),", ",(0,i.jsx)(n.code,{children:"distil-large-v3"}),", ",(0,i.jsx)(n.code,{children:"large-v3-turbo"}),", ",(0,i.jsx)(n.code,{children:"turbo"})]}),"\n",(0,i.jsx)(n.p,{children:"The distil series models may only support English."}),"\n",(0,i.jsxs)(n.p,{children:["The selected model will be automatically downloaded from Hugging Face to the ",(0,i.jsx)(n.code,{children:"models/whisper"})," folder in the project directory."]}),"\n",(0,i.jsxs)(n.p,{children:["Test results on 4060 (Thanks to Lena from the QQ group for providing test results in ",(0,i.jsx)(n.a,{href:"https://github.com/Open-LLM-VTuber/Open-LLM-VTuber/issues/187#issuecomment-2814846254",children:"#187"}),", ",(0,i.jsx)(n.a,{href:"https://github.com/Open-LLM-VTuber/Open-LLM-VTuber/pull/188",children:"#188"}),")"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"Using 22-second generated audio, tested with int8 on 13th gen i5 and 4060 8GB, CUDA 12.8, cuDNN 9.8:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"CPU: v3-turbo took 5.98 seconds, small took 1.56 seconds"}),"\n",(0,i.jsx)(n.li,{children:"GPU: v3-turbo took 1.04 seconds, small took 0.48 seconds"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Summary:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Without 4060, choose small, because medium and v3-turbo are similar in size, small might be the best recognition effect while ensuring speed for 20/30 series cards."}),"\n",(0,i.jsx)(n.li,{children:"With 4060, choose v3-turbo, higher accuracy is naturally better if speed is not an issue."}),"\n",(0,i.jsx)(n.li,{children:"Accuracy reference: faster-whisper-small has 244M parameters, faster-whisper-v3-turbo has 809M parameters."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Test results on MacBook Pro M1 Pro:"}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"Don't even try, it's very slow. Using whisper cpp with CoreML acceleration or sense voice small model would be much faster."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Hugging Face model id format"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'"username/whisper-large-v3-ct2"\n'})}),"\n",(0,i.jsx)(n.p,{children:"Note that faster whisper requires models already converted to CTranslate2 format."}),"\n",(0,i.jsxs)(n.p,{children:["The selected model will be automatically downloaded from Hugging Face to the ",(0,i.jsx)(n.code,{children:"models/whisper"})," folder in the project directory."]}),"\n",(0,i.jsxs)(n.h2,{id:"whisper_cpp-local",children:[(0,i.jsx)(n.code,{children:"whisper_cpp"})," (Local)"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"whisper_cpp"})," can be accelerated through CoreML on macOS for faster inference speed"]}),"\n",(0,i.jsx)(n.li,{children:"When running on CPU or NVIDIA GPU, performance may not be as good as Faster-Whisper"}),"\n",(0,i.jsxs)(n.li,{children:["Mac users please refer to the instructions below to configure WhisperCPP with CoreML support; if you need to use CPU or NVIDIA GPU, just run ",(0,i.jsx)(n.code,{children:"pip install pywhispercpp"})," to install"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"recommended-users-3",children:"Recommended Users"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Mac users who want to utilize GPU inference for Whisper series models"}),"\n",(0,i.jsx)(n.li,{children:"Chinese users"}),"\n",(0,i.jsx)(n.li,{children:"CPU inference is relatively slow, GPU is needed"}),"\n",(0,i.jsx)(n.li,{children:"Configuration difficulty: Setting up GPU acceleration might be a bit challenging"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"SenseVoiceSmall may have average English performance."}),"\n",(0,i.jsx)(n.h3,{id:"installation-1",children:"Installation"}),"\n",(0,i.jsxs)(l.A,{children:[(0,i.jsx)(t.A,{value:"nvidia-gpu",label:"NVIDIA GPU",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"GGML_CUDA=1 uv pip install git+https://github.com/absadiki/pywhispercpp\n"})})}),(0,i.jsx)(t.A,{value:"macos",label:"macOS",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"WHISPER_COREML=1 uv pip install git+https://github.com/absadiki/pywhispercpp\n"})})}),(0,i.jsx)(t.A,{value:"vulkan",label:"Vulkan",children:(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"GGML_VULKAN=1 pip install git+https://github.com/absadiki/pywhispercpp\n"})})})]}),"\n",(0,i.jsx)(n.h3,{id:"coreml-configuration",children:"CoreML Configuration"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Method 1: Follow the Whisper.cpp repository documentation to convert Whisper models to CoreML format"}),"\n",(0,i.jsxs)(n.li,{children:["Method 2: Download pre-converted CoreML models from ",(0,i.jsx)(n.a,{href:"https://huggingface.co/chidiwilliams/whisper.cpp-coreml/tree/main",children:"Hugging Face repository"}),". Note: After downloading, you need to extract the model files, otherwise the program cannot load and will crash."]}),"\n",(0,i.jsxs)(n.li,{children:["Configuration note: When configuring models in ",(0,i.jsx)(n.code,{children:"conf.yaml"}),", you don't need to include the special prefix in the filename. For example, when the CoreML model filename is ",(0,i.jsx)(n.code,{children:"ggml-base-encoder.mlmodelc"}),", you only need to fill in ",(0,i.jsx)(n.code,{children:"base"})," in the ",(0,i.jsx)(n.code,{children:"model_name"})," parameter of ",(0,i.jsx)(n.code,{children:"WhisperCPP"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.h2,{id:"whisper-local",children:[(0,i.jsx)(n.code,{children:"whisper"})," (Local)"]}),"\n",(0,i.jsxs)(n.p,{children:["OpenAI's original Whisper. Install with ",(0,i.jsx)(n.code,{children:"uv pip install -U openai-whisper"}),". Very slow inference speed."]}),"\n",(0,i.jsx)(n.h3,{id:"recommended-users-4",children:"Recommended Users"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Not recommended"}),"\n"]}),"\n",(0,i.jsxs)(n.h2,{id:"groq_whisper_asr-online-requires-api-key-but-easy-to-register-with-generous-free-quota",children:[(0,i.jsx)(n.code,{children:"groq_whisper_asr"})," (Online, requires API key, but easy to register with generous free quota)"]}),"\n",(0,i.jsxs)(n.p,{children:["Groq's Whisper endpoint, very accurate (supports multiple languages) and fast, with many free uses per day. It's pre-installed. Get an API key from ",(0,i.jsx)(n.a,{href:"https://console.groq.com/keys",children:"groq"})," and add it to the ",(0,i.jsx)(n.code,{children:"groq_whisper_asr"})," settings in ",(0,i.jsx)(n.code,{children:"conf.yaml"}),". Users in mainland China and other unsupported regions need a proxy (may not support Hong Kong region) to use it."]}),"\n",(0,i.jsx)(n.h3,{id:"recommended-users-5",children:"Recommended Users"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Users who accept using online speech recognition"}),"\n",(0,i.jsx)(n.li,{children:"Multilingual users"}),"\n",(0,i.jsx)(n.li,{children:"No local computation, very fast speed (depends on your network speed)"}),"\n",(0,i.jsx)(n.li,{children:"Configuration difficulty: Simple"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"SenseVoiceSmall may have average English performance."}),"\n",(0,i.jsxs)(n.h2,{id:"azure_asr-online-requires-api-key",children:[(0,i.jsx)(n.code,{children:"azure_asr"})," (Online, requires API key)"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Azure Speech Recognition"}),"\n",(0,i.jsxs)(n.li,{children:["Configure API key and region under the ",(0,i.jsx)(n.code,{children:"azure_asr"})," option"]}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"api_key.py"})," has been deprecated after ",(0,i.jsx)(n.code,{children:"v0.2.5"}),". Please set API keys in ",(0,i.jsx)(n.code,{children:"conf.yaml"}),"."]})}),"\n",(0,i.jsx)(n.h3,{id:"recommended-users-6",children:"Recommended Users"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"People who have Azure API keys (Azure accounts are not easy to register)"}),"\n",(0,i.jsx)(n.li,{children:"Multilingual users"}),"\n",(0,i.jsx)(n.li,{children:"No local computation, very fast speed (depends on your network speed)"}),"\n",(0,i.jsx)(n.li,{children:"Configuration difficulty: Simple"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(u,{...e})}):u(e)}},9329:(e,n,r)=>{r.d(n,{A:()=>l});r(6540);var s=r(4164);const i={tabItem:"tabItem_Ymn6"};var o=r(4848);function l(e){let{children:n,hidden:r,className:l}=e;return(0,o.jsx)("div",{role:"tabpanel",className:(0,s.A)(i.tabItem,l),hidden:r,children:n})}},5537:(e,n,r)=>{r.d(n,{A:()=>w});var s=r(6540),i=r(4164),o=r(5627),l=r(6347),t=r(372),a=r(604),c=r(1861),d=r(8749);function h(e){return s.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:n,children:r}=e;return(0,s.useMemo)((()=>{const e=n??function(e){return h(e).map((e=>{let{props:{value:n,label:r,attributes:s,default:i}}=e;return{value:n,label:r,attributes:s,default:i}}))}(r);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,r])}function p(e){let{value:n,tabValues:r}=e;return r.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:r}=e;const i=(0,l.W6)(),o=function(e){let{queryString:n=!1,groupId:r}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!r)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return r??null}({queryString:n,groupId:r});return[(0,a.aZ)(o),(0,s.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(i.location.search);n.set(o,e),i.replace({...i.location,search:n.toString()})}),[o,i])]}function x(e){const{defaultValue:n,queryString:r=!1,groupId:i}=e,o=u(e),[l,a]=(0,s.useState)((()=>function(e){let{defaultValue:n,tabValues:r}=e;if(0===r.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!p({value:n,tabValues:r}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${r.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const s=r.find((e=>e.default))??r[0];if(!s)throw new Error("Unexpected error: 0 tabValues");return s.value}({defaultValue:n,tabValues:o}))),[c,h]=m({queryString:r,groupId:i}),[x,f]=function(e){let{groupId:n}=e;const r=function(e){return e?`docusaurus.tab.${e}`:null}(n),[i,o]=(0,d.Dv)(r);return[i,(0,s.useCallback)((e=>{r&&o.set(e)}),[r,o])]}({groupId:i}),j=(()=>{const e=c??x;return p({value:e,tabValues:o})?e:null})();(0,t.A)((()=>{j&&a(j)}),[j]);return{selectedValue:l,selectValue:(0,s.useCallback)((e=>{if(!p({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);a(e),h(e),f(e)}),[h,f,o]),tabValues:o}}var f=r(9136);const j={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var g=r(4848);function v(e){let{className:n,block:r,selectedValue:s,selectValue:l,tabValues:t}=e;const a=[],{blockElementScrollPositionUntilNextRender:c}=(0,o.a_)(),d=e=>{const n=e.currentTarget,r=a.indexOf(n),i=t[r].value;i!==s&&(c(n),l(i))},h=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const r=a.indexOf(e.currentTarget)+1;n=a[r]??a[0];break}case"ArrowLeft":{const r=a.indexOf(e.currentTarget)-1;n=a[r]??a[a.length-1];break}}n?.focus()};return(0,g.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":r},n),children:t.map((e=>{let{value:n,label:r,attributes:o}=e;return(0,g.jsx)("li",{role:"tab",tabIndex:s===n?0:-1,"aria-selected":s===n,ref:e=>{a.push(e)},onKeyDown:h,onClick:d,...o,className:(0,i.A)("tabs__item",j.tabItem,o?.className,{"tabs__item--active":s===n}),children:r??n},n)}))})}function b(e){let{lazy:n,children:r,selectedValue:o}=e;const l=(Array.isArray(r)?r:[r]).filter(Boolean);if(n){const e=l.find((e=>e.props.value===o));return e?(0,s.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,g.jsx)("div",{className:"margin-top--md",children:l.map(((e,n)=>(0,s.cloneElement)(e,{key:n,hidden:e.props.value!==o})))})}function y(e){const n=x(e);return(0,g.jsxs)("div",{className:(0,i.A)("tabs-container",j.tabList),children:[(0,g.jsx)(v,{...n,...e}),(0,g.jsx)(b,{...n,...e})]})}function w(e){const n=(0,f.A)();return(0,g.jsx)(y,{...e,children:h(e.children)},String(n))}},8453:(e,n,r)=>{r.d(n,{R:()=>l,x:()=>t});var s=r(6540);const i={},o=s.createContext(i);function l(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);