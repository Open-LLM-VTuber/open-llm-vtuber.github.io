"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[40],{8656:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"user-guide/backend/tts","title":"Speech Synthesis (TTS)","description":"After installing the required dependencies and configuring conf.yaml, enable the corresponding speech synthesis engine by modifying the TTS_MODEL option in conf.yaml.","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/user-guide/backend/tts.md","sourceDirName":"user-guide/backend","slug":"/user-guide/backend/tts","permalink":"/en/docs/user-guide/backend/tts","draft":false,"unlisted":false,"editUrl":"https://github.com/Open-LLM-VTuber/Open-LLM-VTuber-Docs/tree/main/docs/user-guide/backend/tts.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"docSidebar","previous":{"title":"Agent","permalink":"/en/docs/user-guide/backend/agent"},"next":{"title":"Translation","permalink":"/en/docs/user-guide/backend/translate"}}');var o=i(4848),t=i(8453);const l={sidebar_position:6},r="Speech Synthesis (TTS)",c={},d=[{value:"sherpa-onnx (Local &amp; Recommended)",id:"sherpa-onnx-local--recommended",level:2},{value:"pyttsx3 (Lightweight and Fast)",id:"pyttsx3-lightweight-and-fast",level:2},{value:"MeloTTS (Local Deployment)",id:"melotts-local-deployment",level:2},{value:"Installation Steps",id:"installation-steps",level:3},{value:"Additional Notes",id:"additional-notes",level:3},{value:"Coqui-TTS (Local Deployment)",id:"coqui-tts-local-deployment",level:2},{value:"Installation Steps",id:"installation-steps-1",level:3},{value:"Model Configuration",id:"model-configuration",level:3},{value:"GPTSoVITS (Local Deployment, Moderate Performance)",id:"gptsovits-local-deployment-moderate-performance",level:2},{value:"GPTSoVITS-V2 Integration Package",id:"gptsovits-v2-integration-package",level:3},{value:"miHoYo One-Click Package",id:"mihoyo-one-click-package",level:3},{value:"If you are using the miHoYo one-click package:",id:"if-you-are-using-the-mihoyo-one-click-package",level:4},{value:"If you are using the GPT-SovitsV2 integrated package:",id:"if-you-are-using-the-gpt-sovitsv2-integrated-package",level:4},{value:"Bark (Local Deployment, Relatively Slow)",id:"bark-local-deployment-relatively-slow",level:2},{value:"CosyVoice TTS (Local Deployment, Slower)",id:"cosyvoice-tts-local-deployment-slower",level:2},{value:"CosyVoice2 TTS (Local Deployment)",id:"cosyvoice2-tts-local-deployment",level:2},{value:"X-TTS (Local Deployment, Relatively Slow)",id:"x-tts-local-deployment-relatively-slow",level:2},{value:"Edge TTS (Online, No API Key Required)",id:"edge-tts-online-no-api-key-required",level:2},{value:"Fish Audio TTS (Online, API Key Required)",id:"fish-audio-tts-online-api-key-required",level:2},{value:"Azure TTS (Online, API Key Required)",id:"azure-tts-online-api-key-required",level:2},{value:"SiliconFlow TTS (Online, API Key Required)",id:"siliconflow-tts-online-api-key-required",level:2},{value:"Configuration Steps",id:"configuration-steps",level:3},{value:"MiniMax TTS (Online, API Key Required)",id:"minimax-tts-online-api-key-required",level:2},{value:"Configuration Steps",id:"configuration-steps-1",level:3}];function a(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"speech-synthesis-tts",children:"Speech Synthesis (TTS)"})}),"\n",(0,o.jsxs)(n.p,{children:["After installing the required dependencies and configuring ",(0,o.jsx)(n.code,{children:"conf.yaml"}),", enable the corresponding speech synthesis engine by modifying the ",(0,o.jsx)(n.code,{children:"TTS_MODEL"})," option in ",(0,o.jsx)(n.code,{children:"conf.yaml"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"sherpa-onnx-local--recommended",children:"sherpa-onnx (Local & Recommended)"}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:["Available since version ",(0,o.jsx)(n.code,{children:"v0.5.0-alpha.1"})," (",(0,o.jsx)(n.a,{href:"https://github.com/t41372/Open-LLM-VTuber/pull/50",children:"PR#50"}),")"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"sherpa-onnx is a powerful inference engine that supports multiple TTS models (including MeloTTS). It is built-in supported and uses CPU inference by default."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Configuration Steps:"})}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Download the required model from ",(0,o.jsx)(n.a,{href:"https://github.com/k2-fsa/sherpa-onnx/releases/tag/tts-models",children:"sherpa-onnx TTS models"})]}),"\n",(0,o.jsxs)(n.li,{children:["Modify ",(0,o.jsx)(n.code,{children:"conf.yaml"})," referring to the configuration examples in ",(0,o.jsx)(n.code,{children:"config_alts"})]}),"\n"]}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsxs)(n.p,{children:["For GPU inference (CUDA only), please refer to ",(0,o.jsx)(n.a,{href:"/docs/user-guide/backend/asr#cuda-inference",children:"CUDA Inference"}),"."]})}),"\n",(0,o.jsx)(n.h2,{id:"pyttsx3-lightweight-and-fast",children:"pyttsx3 (Lightweight and Fast)"}),"\n",(0,o.jsxs)(n.p,{children:["A simple and easy-to-use local TTS engine that uses the system's default speech synthesizer. We use ",(0,o.jsx)(n.code,{children:"py3-tts"})," instead of the more famous ",(0,o.jsx)(n.code,{children:"pyttsx3"})," because ",(0,o.jsx)(n.code,{children:"pyttsx3"})," seems unmaintained and failed to run on the test computer."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Configuration Steps:"})}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Install by running ",(0,o.jsx)(n.code,{children:"uv pip install py3-tts"})]}),"\n",(0,o.jsxs)(n.li,{children:["Set ",(0,o.jsx)(n.code,{children:"tts_model: pyttsx3_tts"})," in ",(0,o.jsx)(n.code,{children:"conf.yaml"})]}),"\n"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Install using the command ",(0,o.jsx)(n.code,{children:"uv pip install py3-tts"}),"."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsxs)(n.li,{children:["This TTS engine has no configuration options, simply set ",(0,o.jsx)(n.code,{children:"tts_model: pyttsx3_tts"})," in ",(0,o.jsx)(n.code,{children:"conf.yaml"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.admonition,{type:"info",children:[(0,o.jsx)(n.p,{children:"This package will use the default TTS engine on your system:"}),(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Windows uses the sapi5 engine"}),"\n",(0,o.jsx)(n.li,{children:"macOS uses the nsss engine"}),"\n",(0,o.jsx)(n.li,{children:"Other platforms use the espeak engine"}),"\n"]})]}),"\n",(0,o.jsx)(n.h2,{id:"melotts-local-deployment",children:"MeloTTS (Local Deployment)"}),"\n",(0,o.jsx)(n.admonition,{title:"Important Note",type:"warning",children:(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"It is strongly recommended to use MeloTTS through sherpa-onnx, rather than installing the more complex official version"}),"\n",(0,o.jsx)(n.li,{children:"MeloTTS has dependency conflicts with Coqui-TTS. Please do not install them simultaneously"}),"\n",(0,o.jsx)(n.li,{children:"The official version of MeloTTS may encounter mps-related errors on macOS (solutions are welcome)"}),"\n"]})}),"\n",(0,o.jsx)(n.h3,{id:"installation-steps",children:"Installation Steps"}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:["Starting from project version ",(0,o.jsx)(n.code,{children:"v1.0.0"}),", we use ",(0,o.jsx)(n.code,{children:"uv"})," to manage dependencies, which greatly simplifies the installation process of MeloTTS."]}),"\n"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Install MeloTTS and necessary components:"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-sh",children:"# Install MeloTTS\nuv add git+https://github.com/myshell-ai/MeloTTS.git\n\n# Download unidic\npython -m unidic download\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:"Download additional dependencies:"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-sh",children:"# Enter Python interpreter\npython\n\n# Download necessary NLTK data\n>>> import nltk\n>>> nltk.download('averaged_perceptron_tagger_eng')\n# Press Ctrl+D to exit the interpreter when finished\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsx)(n.li,{children:"Configure and enable:"}),"\n"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Edit the project's ",(0,o.jsx)(n.code,{children:"conf.yaml"})," file"]}),"\n",(0,o.jsxs)(n.li,{children:["Set ",(0,o.jsx)(n.code,{children:"tts_model"})," to ",(0,o.jsx)(n.code,{children:"melo_tts"})]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"additional-notes",children:"Additional Notes"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Official documentation: ",(0,o.jsx)(n.a,{href:"https://github.com/myshell-ai/MeloTTS/blob/main/docs/install.md",children:"MeloTTS Installation Guide"})]}),"\n",(0,o.jsxs)(n.li,{children:["If encountering ",(0,o.jsx)(n.code,{children:"mecab-python"})," related issues, try using this ",(0,o.jsx)(n.a,{href:"https://github.com/polm/MeloTTS",children:"branch"})," (Note: As of 2024/7/16, it has not been merged into the main branch)"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"coqui-tts-local-deployment",children:"Coqui-TTS (Local Deployment)"}),"\n",(0,o.jsx)(n.admonition,{title:"Important Note",type:"warning",children:(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Coqui-TTS has dependency conflicts with MeloTTS. Please do not install them simultaneously"}),"\n"]})}),"\n",(0,o.jsx)(n.p,{children:"Coqui-TTS is an open-source speech synthesis toolkit that supports multiple models and languages. The inference speed depends on the size and complexity of the chosen model."}),"\n",(0,o.jsx)(n.h3,{id:"installation-steps-1",children:"Installation Steps"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-sh",children:'# Install Coqui-TTS and its language support\nuv add transformers "coqui-tts[languages]"\n'})}),"\n",(0,o.jsx)(n.h3,{id:"model-configuration",children:"Model Configuration"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"View available models:"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-sh",children:"uv run tts --list_models\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsxs)(n.li,{children:["Configure in ",(0,o.jsx)(n.code,{children:"conf.yaml"}),":"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:' coqui_tts:\n      # Name of the TTS model to use. If empty, the default model will be used\n      # Run "tts --list_models" to list models supported by coqui-tts\n      # Some examples:\n      # - "tts_models/en/ljspeech/tacotron2-DDC" (single speaker)\n      # - "tts_models/zh-CN/baker/tacotron2-DDC-GST" (Chinese single speaker)\n      # - "tts_models/multilingual/multi-dataset/your_tts" (multi-speaker)\n      # - "tts_models/multilingual/multi-dataset/xtts_v2" (multi-speaker)\n      model_name: "tts_models/en/ljspeech/tacotron2-DDC" # Model name\n      speaker_wav: "" # Path to reference audio file\n      language: "en" # Language\n      device: "" # Device\n'})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Single Language Models"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Default configuration is for English single language model"}),"\n",(0,o.jsxs)(n.li,{children:["For Chinese support, please change to a Chinese model (e.g., ",(0,o.jsx)(n.code,{children:"tts_models/zh-CN/baker/tacotron2-DDC-GST"}),")"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Multilingual Models"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"speaker_wav"}),": Path to reference audio file","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Supports relative paths (e.g., ",(0,o.jsx)(n.code,{children:"./voices/reference.wav"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:["For Windows, when using absolute paths, change ",(0,o.jsx)(n.code,{children:"\\"})," to ",(0,o.jsx)(n.code,{children:"\\\\"})]}),"\n",(0,o.jsx)(n.li,{children:"Ensure the reference audio file exists at the specified location"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"language"}),": Set the preferred language","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Set to ",(0,o.jsx)(n.code,{children:'"zh"'})," for Chinese"]}),"\n",(0,o.jsxs)(n.li,{children:["Set to ",(0,o.jsx)(n.code,{children:'"en"'})," for English"]}),"\n",(0,o.jsxs)(n.li,{children:["This parameter corresponds to ",(0,o.jsx)(n.code,{children:"speaker_wav"})]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"gptsovits-local-deployment-moderate-performance",children:"GPTSoVITS (Local Deployment, Moderate Performance)"}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:["Introduced in ",(0,o.jsx)(n.a,{href:"https://github.com/t41372/Open-LLM-VTuber/pull/40",children:"PR #40"}),", officially released in version v0.4.0"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"GPT-SoVITS is a powerful speech synthesis engine capable of high-quality voice cloning."}),"\n",(0,o.jsxs)(n.admonition,{type:"note",children:[(0,o.jsxs)(n.p,{children:["Note: The official tutorial for GPTSoVITS is currently not comprehensive.\nSome of the following content has been compiled from the ",(0,o.jsx)(n.a,{href:"https://docs.qq.com/doc/DTHR6WkZ3aU9JcXpy",children:"Tencent Document"}),", which was created by the users from the qq group before the release of ",(0,o.jsx)(n.code,{children:"v1.0.0"})," (the doc was blocked by tencent for some reason so it is no longer maintained)."]}),(0,o.jsx)(n.p,{children:"If you encountered any problems or would like to improve the documentation, please use the edit button at the end of this page or just contact me."})]}),"\n",(0,o.jsx)(n.h3,{id:"gptsovits-v2-integration-package",children:(0,o.jsx)(n.a,{href:"https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e/dkxgpiy9zb96hob4#KTvnO",children:"GPTSoVITS-V2 Integration Package"})}),"\n",(0,o.jsx)(n.h3,{id:"mihoyo-one-click-package",children:(0,o.jsx)(n.a,{href:"https://www.bilibili.com/video/BV1D7421R7Rn",children:"miHoYo One-Click Package"})}),"\n",(0,o.jsx)(n.h4,{id:"if-you-are-using-the-mihoyo-one-click-package",children:"If you are using the miHoYo one-click package:"}),"\n",(0,o.jsxs)(n.p,{children:["First launch GPT SoVITS, then launch this project (",(0,o.jsx)(n.code,{children:"uv run run_server.py"}),")."]}),"\n",(0,o.jsx)(n.p,{children:"You need to modify the following settings:"}),"\n",(0,o.jsxs)(n.p,{children:["1: Change the tts option in conf.yaml to gpt_sovits (surely no one would overlook this step)\n",(0,o.jsx)(n.img,{src:i(6160).A+"",width:"1340",height:"576"})]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["This screenshot was taken before the release of ",(0,o.jsx)(n.code,{children:"v1.0.0"}),". Please enter ",(0,o.jsx)(n.code,{children:"gpt_sovits"})," instead of ",(0,o.jsx)(n.code,{children:"GPT_Sovits"}),"."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["2: Modify the corresponding configuration parameters in gpt_sovits below:\n",(0,o.jsx)(n.img,{src:i(9659).A+"",width:"1288",height:"480"}),"\nfrom top to bottom, the red text in the image says:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"The url of the GPT sovits server endpoint"}),"\n",(0,o.jsxs)(n.li,{children:["please have the ",(0,o.jsx)(n.code,{children:"/tts"})," at the end of the url"]}),"\n",(0,o.jsxs)(n.li,{children:["change the ",(0,o.jsx)(n.code,{children:"ref_audio_path"})," to the path of the reference audio of the model you use."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["If it prompts that GPT-Sovits loaded successfully but ffmpeg reports a decoding failure, it's because you didn't add /tts:\n",(0,o.jsx)(n.img,{src:i(8530).A+"",width:"1958",height:"1584"})]}),"\n",(0,o.jsx)(n.h4,{id:"if-you-are-using-the-gpt-sovitsv2-integrated-package",children:"If you are using the GPT-SovitsV2 integrated package:"}),"\n",(0,o.jsxs)(n.p,{children:["1: The modifications in conf.yaml are the same as the previous step, but place the corresponding models in the appropriate locations. Put the GPT model (with .ckpt extension) in the GPT_weights_v2 folder, the SoVITS model (with .pth extension) in the SoVITS_weights_v2 folder. If you don't change the location of the reference audio, it should be placed in the GPT-Sovits root directory, alongside api_v2.py;\n",(0,o.jsx)(n.img,{src:i(4877).A+"",width:"640",height:"203"}),"\n",(0,o.jsx)(n.img,{src:i(3748).A+"",width:"653",height:"246"}),"\n",(0,o.jsx)(n.img,{src:i(2655).A+"",width:"903",height:"203"})]}),"\n",(0,o.jsxs)(n.p,{children:["2: Run GPT-SovitsV2, navigate to the GPT-Sovits root directory, and in the prompt run ",(0,o.jsx)(n.code,{children:"python api_v2.py -a 0.0.0.0"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["If there's no response, use the Python that comes with the integrated package, and in the prompt run ",(0,o.jsx)(n.code,{children:"runtime\\python.exe api_v2.py"}),'. When it prompts "TTS config", it indicates that it has loaded successfully. You can then leave it running in the background.']}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:i(9990).A+"",width:"979",height:"603"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"the red text says: this is the url you need to put into conf.yaml"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"bark-local-deployment-relatively-slow",children:"Bark (Local Deployment, Relatively Slow)"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Install dependencies:","\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-sh",children:"uv pip install git+https://github.com/suno-ai/bark.git\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["Set ",(0,o.jsx)(n.code,{children:"tts_model: bark_tts"})," in ",(0,o.jsx)(n.code,{children:"conf.yaml"})]}),"\n",(0,o.jsx)(n.li,{children:"Required models will be automatically downloaded on first launch"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"cosyvoice-tts-local-deployment-slower",children:"CosyVoice TTS (Local Deployment, Slower)"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Configure and start WebUI according to ",(0,o.jsx)(n.a,{href:"https://github.com/FunAudioLLM/CosyVoice",children:"CosyVoice Official Documentation"})]}),"\n",(0,o.jsxs)(n.li,{children:["Refer to the API documentation in WebUI to configure the ",(0,o.jsx)(n.code,{children:"cosyvoice_tts"})," section in ",(0,o.jsx)(n.code,{children:"conf.yaml"})]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"cosyvoice2-tts-local-deployment",children:"CosyVoice2 TTS (Local Deployment)"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Set up the environment according to ",(0,o.jsx)(n.a,{href:"https://github.com/FunAudioLLM/CosyVoice",children:"CosyVoice Official Documentation"})]}),"\n",(0,o.jsxs)(n.li,{children:["Download CosyVoice2 model ",(0,o.jsx)(n.code,{children:"CosyVoice2-0.5B"})]}),"\n",(0,o.jsxs)(n.li,{children:["Modify CosyVoice's ",(0,o.jsx)(n.code,{children:"webui.py"})," file","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'audio_output = gr.Audio(label="Synthesized Audio", autoplay=True, streaming=True) \nchange to ->\naudio_output = gr.Audio(label="Synthesized Audio", autoplay=True, streaming=False)\n'})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"   logging.info('get instruct inference request')\n   set_all_random_seed(seed)\n   for i in cosyvoice.inference_instruct(tts_text, sft_dropdown, instruct_text, stream=stream, speed=speed):\n       yield (cosyvoice.sample_rate, i['tts_speech'].numpy().flatten())\nchange to ->\n        logging.info('get instruct inference request')\n        prompt_speech_16k = postprocess(load_wav(prompt_wav, prompt_sr))\n        set_all_random_seed(seed)\n        for i in cosyvoice.inference_instruct2(tts_text, instruct_text, prompt_speech_16k, stream=stream, speed=speed):\n            yield (cosyvoice.sample_rate, i['tts_speech'].numpy().flatten())\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.li,{children:"Start CosyVoice WebUI"}),"\n",(0,o.jsxs)(n.li,{children:["Install gradio_client in this project using the command ",(0,o.jsx)(n.code,{children:"uv add gradio_client"})]}),"\n",(0,o.jsxs)(n.li,{children:["Configure the ",(0,o.jsx)(n.code,{children:"cosyvoice2_tts"})," section in ",(0,o.jsx)(n.code,{children:"conf.yaml"})]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Currently, only the base model of CosyVoice2 - ",(0,o.jsx)(n.code,{children:"CosyVoice2-0.5B"}),' has been released, which only supports "3s Quick Voice Cloning", "Cross-lingual Voice Cloning", and "Natural Language Control"']}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'In "3s Quick Voice Cloning" mode, you need to fill in prompt_wav_upload_url and prompt_wav_record_url as reference audio, and prompt_text as the corresponding text for the reference audio'}),"\n",(0,o.jsx)(n.li,{children:'In "Cross-lingual Voice Cloning" mode, you need to fill in prompt_wav_upload_url and prompt_wav_record_url as reference audio. Best results are achieved when the generated audio language differs from the reference audio language'}),"\n",(0,o.jsx)(n.li,{children:'In "Natural Language Control" mode, you need to fill in prompt_wav_upload_url and prompt_wav_record_url as reference audio, and instruct_text as the control instruction, such as "speak Cantonese", "use an enthusiastic tone"'}),"\n"]}),"\n",(0,o.jsxs)(n.admonition,{type:"warning",children:[(0,o.jsx)(n.mdxAdmonitionTitle,{}),(0,o.jsx)(n.p,{children:"Please fill in prompt_wav_upload_url and prompt_wav_record_url with the same path\nIt's recommended to set stream (streaming generation) to False, as this project already includes voice segment synthesis (definitely not because setting it to True causes bugs)"})]}),"\n",(0,o.jsx)(n.h2,{id:"x-tts-local-deployment-relatively-slow",children:"X-TTS (Local Deployment, Relatively Slow)"}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:["Available since version ",(0,o.jsx)(n.code,{children:"v0.2.4"})," (",(0,o.jsx)(n.a,{href:"https://github.com/t41372/Open-LLM-VTuber/pull/23",children:"PR#23"}),")"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"It is recommended to use xtts-api-server, which provides clear API documentation and is relatively easy to deploy."}),"\n",(0,o.jsx)(n.h2,{id:"edge-tts-online-no-api-key-required",children:"Edge TTS (Online, No API Key Required)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Features:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Fast response speed"}),"\n",(0,o.jsx)(n.li,{children:"Requires maintaining network connection"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["Configuration: Set ",(0,o.jsx)(n.code,{children:"tts_model: edge_tts"})," in ",(0,o.jsx)(n.code,{children:"conf.yaml"})]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"fish-audio-tts-online-api-key-required",children:"Fish Audio TTS (Online, API Key Required)"}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:["Available since version ",(0,o.jsx)(n.code,{children:"v0.3.0-beta"})]}),"\n"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Install dependencies:"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-sh",children:"uv pip install fish-audio-sdk\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsxs)(n.li,{children:["Configuration steps:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Register an account on ",(0,o.jsx)(n.a,{href:"https://fish.audio/",children:"Fish Audio"})," and obtain an API key"]}),"\n",(0,o.jsx)(n.li,{children:"Select the desired voice and copy its Reference ID"}),"\n",(0,o.jsxs)(n.li,{children:["In ",(0,o.jsx)(n.code,{children:"conf.yaml"}),", set:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"tts_model: fish_api_tts"})}),"\n",(0,o.jsxs)(n.li,{children:["Fill in ",(0,o.jsx)(n.code,{children:"api_key"})," and ",(0,o.jsx)(n.code,{children:"reference_id"})," in the ",(0,o.jsx)(n.code,{children:"fish_api_tts"})," section"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"azure-tts-online-api-key-required",children:"Azure TTS (Online, API Key Required)"}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsx)(n.p,{children:"The same TTS service as neuro-sama"}),"\n"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Obtain an API key for the text-to-speech service from Azure"}),"\n",(0,o.jsxs)(n.li,{children:["Fill in the relevant configuration in the ",(0,o.jsx)(n.code,{children:"azure_tts"})," section of ",(0,o.jsx)(n.code,{children:"conf.yaml"})]}),"\n"]}),"\n",(0,o.jsx)(n.admonition,{type:"warning",children:(0,o.jsxs)(n.p,{children:["Since version ",(0,o.jsx)(n.code,{children:"v0.2.5"}),", ",(0,o.jsx)(n.code,{children:"api_key.py"})," has been deprecated. Please make sure to set the API key in ",(0,o.jsx)(n.code,{children:"conf.yaml"})]})}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsxs)(n.p,{children:["The default voice used in ",(0,o.jsx)(n.code,{children:"conf.yaml"})," is the same as neuro-sama"]})}),"\n",(0,o.jsx)(n.h2,{id:"siliconflow-tts-online-api-key-required",children:"SiliconFlow TTS (Online, API Key Required)"}),"\n",(0,o.jsx)(n.p,{children:"An online text-to-speech service provided by SiliconFlow, supporting custom audio models and voice configuration."}),"\n",(0,o.jsx)(n.h3,{id:"configuration-steps",children:"Configuration Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Upload Reference Audio"}),"\uff1a",(0,o.jsx)(n.br,{}),"\n","SiliconFlow currently offers models like ",(0,o.jsx)(n.code,{children:"FunAudioLLM/CosyVoice2-0.5B"}),". To use them, upload reference audio via their official platform:",(0,o.jsx)(n.br,{}),"\n",(0,o.jsx)(n.a,{href:"https://docs.siliconflow.cn/cn/api-reference/audio/upload-voice",children:"https://docs.siliconflow.cn/cn/api-reference/audio/upload-voice"})]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsxs)(n.strong,{children:["Fill in ",(0,o.jsx)(n.code,{children:"conf.yaml"})]}),"\uff1a",(0,o.jsx)(n.br,{}),"\n","In the ",(0,o.jsx)(n.code,{children:"siliconflow_tts"})," section of the configuration file, configure parameters as follows (example):"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'siliconflow_tts:\n  api_url: "https://api.siliconflow.cn/v1/audio/speech"  # Service endpoint (fixed value)\n  api_key: "sk-yourkey"  # API key obtained from SiliconFlow\'s official website\n  default_model: "FunAudioLLM/CosyVoice2-0.5B"  # Audio model name (check official docs for supported models)\n  default_voice: "speech:Dreamflowers:aaaaaaabvbbbasdas"  # Voice ID (generated after uploading custom voice on the official site)\n  sample_rate: 32000  # Output sample rate; adjust if audio is distorted (e.g., 16000, 44100)\n  response_format: "mp3"  # Audio format (e.g., mp3, wav)\n  stream: true  # Enable streaming mode\n  speed: 1  # Speaking speed (range: 0.5\u20132.0; 1 = default)\n  gain: 0  # Volume gain (range: -10\u201310; 0 = default)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"minimax-tts-online-api-key-required",children:"MiniMax TTS (Online, API Key Required)"}),"\n",(0,o.jsxs)(n.p,{children:["MiniMax provides an online TTS service where models like ",(0,o.jsx)(n.code,{children:"speech-02-turbo"})," offer powerful TTS capabilities with customizable voice options."]}),"\n",(0,o.jsx)(n.h3,{id:"configuration-steps-1",children:"Configuration Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsxs)(n.strong,{children:["Obtain ",(0,o.jsx)(n.code,{children:"group_id"})," and ",(0,o.jsx)(n.code,{children:"api_key"})]}),"\nYou can register on the Minimax official website to get your ",(0,o.jsx)(n.code,{children:"group_id"})," and ",(0,o.jsx)(n.code,{children:"api_key"}),", ",(0,o.jsx)(n.a,{href:"https://platform.minimaxi.com/document/Fast",children:"Official Documentation"})]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsxs)(n.strong,{children:["Fill in the ",(0,o.jsx)(n.code,{children:"conf.yaml"})," configuration"]}),"\nIn the ",(0,o.jsx)(n.code,{children:"minimax_tts"})," section of the configuration file, enter parameters in the following format (example):"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"minimax_tts:\n      group_id: '' # Your minimax group_id\n      api_key: '' # Your minimax api_key\n      # Supported models: 'speech-02-hd', 'speech-02-turbo' (recommended: 'speech-02-turbo')\n      model: 'speech-02-turbo' # minimax model name\n      voice_id: 'female-shaonv' # minimax voice id, default is 'female-shaonv'\n      # Custom pronunciation dictionary, default empty.\n      # Example: '{\"tone\": [\"\u6d4b\u8bd5/(ce4)(shi4)\", \"\u5371\u9669/dangerous\"]}'\n      pronunciation_dict: ''\n"})}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"voice_id"})," parameter can be configured to different voice tones. You can check the ",(0,o.jsx)(n.a,{href:"https://platform.minimaxi.com/document/get_voice",children:"voice ID query section in the official documentation"})," for a complete list of supported voices. The ",(0,o.jsx)(n.code,{children:"pronunciation_dict"}),' supports custom pronunciation rules - for example, you can define rules to pronounce "\u725b\u8089" as "neuro" using the format shown in the example.']})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(a,{...e})}):a(e)}},6160:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/gpt_sovits_1-0095dfb4d736cfafff39a06ec73909c9.png"},9659:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/gpt_sovits_2-7a06cacc1f7a47b2b512a48127c8a5bd.png"},8530:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/gpt_sovits_3-0d64562e65d7b8865370c7b8d7258ea4.png"},4877:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/gpt_sovits_4-f2af17262fca2652f816d2d92efb77f6.png"},3748:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/gpt_sovits_5-74fb34cdb8f377def13fccba89eeda23.png"},2655:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/gpt_sovits_6-d17c6ff286a5098a2021b6db7fc54c43.png"},9990:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/gpt_sovits_7-9219682950050258824c0fed61631e79.png"},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>r});var s=i(6540);const o={},t=s.createContext(o);function l(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:l(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);