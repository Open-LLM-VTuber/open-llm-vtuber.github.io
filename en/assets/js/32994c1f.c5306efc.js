"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1532],{6297:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"user-guide/backend/agent","title":"Agent","description":"An Agent is an LLM system that includes memory, tools, and personality. The default option in the current version is basicmemoryagent.","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/user-guide/backend/agent.md","sourceDirName":"user-guide/backend","slug":"/user-guide/backend/agent","permalink":"/en/docs/user-guide/backend/agent","draft":false,"unlisted":false,"editUrl":"https://github.com/Open-LLM-VTuber/Open-LLM-VTuber-Docs/tree/main/docs/user-guide/backend/agent.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"docSidebar","previous":{"title":"Language Models (LLM)","permalink":"/en/docs/user-guide/backend/llm"},"next":{"title":"Speech Synthesis (TTS)","permalink":"/en/docs/user-guide/backend/tts"}}');var o=i(4848),s=i(8453);const r={sidebar_position:5},a="Agent",c={},l=[{value:"Basic Memory Agent",id:"basic-memory-agent",level:2},{value:"HumeAI Agent (EVI)",id:"humeai-agent-evi",level:2},{value:"Introduction",id:"introduction",level:3},{value:"Configuration Instructions",id:"configuration-instructions",level:3},{value:"Frequently Asked Questions",id:"frequently-asked-questions",level:3},{value:"Mem0 Agent (Experimental)",id:"mem0-agent-experimental",level:2},{value:"Introduction",id:"introduction-1",level:3},{value:"Advantages",id:"advantages",level:4},{value:"Limitations",id:"limitations",level:4}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"agent",children:"Agent"})}),"\n",(0,o.jsxs)(n.p,{children:["An Agent is an LLM system that includes memory, tools, and personality. The default option in the current version is ",(0,o.jsx)(n.code,{children:"basic_memory_agent"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["Agent-related settings are under ",(0,o.jsx)(n.code,{children:"agent_config"})," in ",(0,o.jsx)(n.code,{children:"conf.yaml"}),". You can modify the agent you're conversing with by changing the ",(0,o.jsx)(n.code,{children:"conversation_agent_choice"}),"."]}),"\n",(0,o.jsx)(n.p,{children:"Currently, this project implements the following agents:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Basic Memory Agent"}),"\n",(0,o.jsx)(n.li,{children:"HumeAI Agent (EVI)"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"basic-memory-agent",children:"Basic Memory Agent"}),"\n",(0,o.jsx)(n.p,{children:"Basic Memory Agent is the project's default, featuring short-term memory, conversation record storage/switching, and other capabilities."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'basic_memory_agent:\n  # Basic AI agent, nothing special.\n  # Choose an LLM provider from llm_config\n  # and set the required parameters in the corresponding field\n  # For example:\n  # "openai_compatible_llm", "llama_cpp_llm", "claude_llm", "ollama_llm"\n  # "openai_llm", "gemini_llm", "zhipu_llm", "deepseek_llm", "groq_llm"\n  llm_provider: "openai_compatible_llm" # LLM solution used\n  # Whether to generate audio immediately upon encountering a comma in the first response to reduce initial delay (default: True)\n  faster_first_response: True\n'})}),"\n",(0,o.jsxs)(n.p,{children:["You can switch between different large language model backends by modifying ",(0,o.jsx)(n.code,{children:"llm_provider"}),". The specific configuration items for each large language model (such as model selection, API Key, etc.) are located under the ",(0,o.jsx)(n.code,{children:"llm_configs"})," configuration block."]}),"\n",(0,o.jsxs)(n.p,{children:["For detailed configuration instructions for each large language model, please refer to ",(0,o.jsx)(n.a,{href:"/docs/user-guide/backend/llm.md",children:"Large Language Model Configuration"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"humeai-agent-evi",children:"HumeAI Agent (EVI)"}),"\n",(0,o.jsx)(n.h3,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"Hume AI's EVI (Empathic Voice Interface) is the world's first voice AI interface with emotional intelligence."}),"\n",(0,o.jsx)(n.p,{children:"It can measure subtle changes in speech and respond through an empathic large language model (eLLM), which guides language and speech generation. Trained on millions of human interactions, it combines language modeling and text-to-speech with better emotional intelligence (EQ), prosody, turn-end detection, interruptibility, and alignment."}),"\n",(0,o.jsxs)(n.p,{children:["New users get $20 of free credit without needing to bind a card. Registration and API access ",(0,o.jsx)(n.strong,{children:"require a proxy in mainland China"}),"."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://www.hume.ai/",children:"Hume AI Official Website"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://dev.hume.ai/intro",children:"Hume AI Developer Documentation"})}),"\n"]}),"\n",(0,o.jsxs)(n.admonition,{type:"note",children:[(0,o.jsxs)(n.p,{children:["To ensure system architecture consistency and compatibility, when using EVI, ASR is still required to transcribe speech to text before processing. This choice allows EVI to seamlessly integrate into the existing ",(0,o.jsx)(n.code,{children:"conversation_chain"})," process, but it also means that EVI's unique audio emotion analysis capabilities cannot be fully utilized."]}),(0,o.jsx)(n.p,{children:"We plan to implement a new architecture in the future to fully support APIs like EVI that have real-time interaction features such as interruption detection, ASR, TTS, and camera support. Contributions from developers are welcome."})]}),"\n",(0,o.jsx)(n.admonition,{title:"EVI Memory Management",type:"info",children:(0,o.jsx)(n.p,{children:"EVI's conversation memory is stored on its server side, meaning it cannot access chat histories of other Agents and can only manage and switch its own conversation history."})}),"\n",(0,o.jsx)(n.h3,{id:"configuration-instructions",children:"Configuration Instructions"}),"\n",(0,o.jsxs)(n.p,{children:["Configure HumeAI Agent in ",(0,o.jsx)(n.code,{children:"conf.yaml"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'agent_config:\n  conversation_agent_choice: "hume_ai_agent"\n  \n  agent_settings:\n    hume_ai_agent:\n      api_key: "your_api_key"        # Hume AI API key\n      config_id: "your_config_id"    # Optional, used to specify a particular Hume AI configuration.\n      host: "api.hume.ai"            # API address provided by HumeAI, no need to modify.\n      idle_timeout: 15               # Automatically disconnect from HumeAI after being idle for this many seconds.\n'})}),"\n",(0,o.jsxs)(n.p,{children:["You can obtain the ",(0,o.jsx)(n.code,{children:"api_key"})," from the ",(0,o.jsx)(n.a,{href:"https://platform.hume.ai/settings/keys",children:"API keys page"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["For detailed explanations about ",(0,o.jsx)(n.code,{children:"config_id"})," and other configurations, please refer to ",(0,o.jsx)(n.a,{href:"https://dev.hume.ai/docs/empathic-voice-interface-evi/configuration",children:"Configuring EVI"}),"."]}),"\n",(0,o.jsx)(n.admonition,{type:"info",children:(0,o.jsxs)(n.p,{children:["The LLM selection and Prompt for EVI need to be set in the configuration corresponding to ",(0,o.jsx)(n.code,{children:"config_id"}),". The Prompt set in ",(0,o.jsx)(n.code,{children:"persona_prompt"})," is not effective for EVI."]})}),"\n",(0,o.jsx)(n.admonition,{type:"warning",children:(0,o.jsxs)(n.p,{children:["Since EVI continues to charge when the WebSocket is idle, it is recommended not to set ",(0,o.jsx)(n.code,{children:"idle_timeout"})," too large. However, also note that if ",(0,o.jsx)(n.code,{children:"idle_timeout"})," is set too small, it will cause frequent disconnections and reconnections, thereby increasing response latency. Therefore, set this parameter reasonably based on actual usage."]})}),"\n",(0,o.jsx)(n.h3,{id:"frequently-asked-questions",children:"Frequently Asked Questions"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://dev.hume.ai/docs/empathic-voice-interface-evi/faq",children:"Empathic Voice Interface FAQ"})}),"\n",(0,o.jsx)(n.h2,{id:"mem0-agent-experimental",children:"Mem0 Agent (Experimental)"}),"\n",(0,o.jsx)(n.h3,{id:"introduction-1",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"Mem0 is an experimental long-term memory solution. Although it's still under development and may not be very suitable for this project's use case, we keep its code for reference."}),"\n",(0,o.jsx)(n.h4,{id:"advantages",children:"Advantages"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Simpler configuration compared to MemGPT"}),"\n",(0,o.jsx)(n.li,{children:"Faster processing than MemGPT (but still consumes a significant amount of LLM tokens)"}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"limitations",children:"Limitations"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Can only remember user preferences and ideas, unable to remember LLM's response content"}),"\n",(0,o.jsx)(n.li,{children:"Unstable triggering mechanism for memory storage"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);